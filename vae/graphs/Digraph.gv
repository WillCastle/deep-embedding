digraph {
	graph [size="122.39999999999999,122.39999999999999"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2165915411424 [label="
 ()" fillcolor=darkolivegreen1]
	2165916202752 [label=MeanBackward0]
	2165916203040 -> 2165916202752
	2165916203040 [label=ConvolutionBackward0]
	2165916202896 -> 2165916203040
	2165916202896 [label=UpsampleNearest2DBackward1]
	2165916203136 -> 2165916202896
	2165916203136 [label=ReluBackward0]
	2165916203232 -> 2165916203136
	2165916203232 [label=AddBackward0]
	2165916203328 -> 2165916203232
	2165916203328 [label=NativeBatchNormBackward0]
	2165916203472 -> 2165916203328
	2165916203472 [label=ConvolutionBackward0]
	2165916203664 -> 2165916203472
	2165916203664 [label=ReluBackward0]
	2165916203808 -> 2165916203664
	2165916203808 [label=NativeBatchNormBackward0]
	2165916203904 -> 2165916203808
	2165916203904 [label=ConvolutionBackward0]
	2165916203280 -> 2165916203904
	2165916203280 [label=ReluBackward0]
	2165916204192 -> 2165916203280
	2165916204192 [label=AddBackward0]
	2165916204288 -> 2165916204192
	2165916204288 [label=NativeBatchNormBackward0]
	2165916204432 -> 2165916204288
	2165916204432 [label=ConvolutionBackward0]
	2165916204624 -> 2165916204432
	2165916204624 [label=ReluBackward0]
	2165916204768 -> 2165916204624
	2165916204768 [label=NativeBatchNormBackward0]
	2165916204864 -> 2165916204768
	2165916204864 [label=ConvolutionBackward0]
	2165916204240 -> 2165916204864
	2165916204240 [label=ReluBackward0]
	2165916729504 -> 2165916204240
	2165916729504 [label=AddBackward0]
	2165916729600 -> 2165916729504
	2165916729600 [label=NativeBatchNormBackward0]
	2165916729744 -> 2165916729600
	2165916729744 [label=ConvolutionBackward0]
	2165916729936 -> 2165916729744
	2165916729936 [label=ReluBackward0]
	2165916730080 -> 2165916729936
	2165916730080 [label=NativeBatchNormBackward0]
	2165916730176 -> 2165916730080
	2165916730176 [label=ConvolutionBackward0]
	2165916729552 -> 2165916730176
	2165916729552 [label=ReluBackward0]
	2165916730464 -> 2165916729552
	2165916730464 [label=AddBackward0]
	2165916730560 -> 2165916730464
	2165916730560 [label=NativeBatchNormBackward0]
	2165916730704 -> 2165916730560
	2165916730704 [label=ConvolutionBackward0]
	2165916730896 -> 2165916730704
	2165916730896 [label=UpsampleNearest2DBackward1]
	2165916731040 -> 2165916730896
	2165916731040 [label=ReluBackward0]
	2165916731136 -> 2165916731040
	2165916731136 [label=NativeBatchNormBackward0]
	2165916731232 -> 2165916731136
	2165916731232 [label=ConvolutionBackward0]
	2165916731424 -> 2165916731232
	2165916731424 [label=ReluBackward0]
	2165916731568 -> 2165916731424
	2165916731568 [label=AddBackward0]
	2165916731664 -> 2165916731568
	2165916731664 [label=NativeBatchNormBackward0]
	2165916731808 -> 2165916731664
	2165916731808 [label=ConvolutionBackward0]
	2165916732000 -> 2165916731808
	2165916732000 [label=ReluBackward0]
	2165916732144 -> 2165916732000
	2165916732144 [label=NativeBatchNormBackward0]
	2165916732240 -> 2165916732144
	2165916732240 [label=ConvolutionBackward0]
	2165916731616 -> 2165916732240
	2165916731616 [label=ReluBackward0]
	2165916732528 -> 2165916731616
	2165916732528 [label=AddBackward0]
	2165916732624 -> 2165916732528
	2165916732624 [label=NativeBatchNormBackward0]
	2165916732768 -> 2165916732624
	2165916732768 [label=ConvolutionBackward0]
	2165916732960 -> 2165916732768
	2165916732960 [label=UpsampleNearest2DBackward1]
	2165916733104 -> 2165916732960
	2165916733104 [label=ReluBackward0]
	2165916733200 -> 2165916733104
	2165916733200 [label=NativeBatchNormBackward0]
	2165916733296 -> 2165916733200
	2165916733296 [label=ConvolutionBackward0]
	2165916733488 -> 2165916733296
	2165916733488 [label=ReluBackward0]
	2165916733632 -> 2165916733488
	2165916733632 [label=AddBackward0]
	2165916733728 -> 2165916733632
	2165916733728 [label=NativeBatchNormBackward0]
	2165916733872 -> 2165916733728
	2165916733872 [label=ConvolutionBackward0]
	2165916734064 -> 2165916733872
	2165916734064 [label=ReluBackward0]
	2165916734208 -> 2165916734064
	2165916734208 [label=NativeBatchNormBackward0]
	2165916734304 -> 2165916734208
	2165916734304 [label=ConvolutionBackward0]
	2165916733680 -> 2165916734304
	2165916733680 [label=ReluBackward0]
	2165916734592 -> 2165916733680
	2165916734592 [label=AddBackward0]
	2165916734688 -> 2165916734592
	2165916734688 [label=NativeBatchNormBackward0]
	2165916734832 -> 2165916734688
	2165916734832 [label=ConvolutionBackward0]
	2165916735024 -> 2165916734832
	2165916735024 [label=UpsampleNearest2DBackward1]
	2165916735168 -> 2165916735024
	2165916735168 [label=ReluBackward0]
	2165916735264 -> 2165916735168
	2165916735264 [label=NativeBatchNormBackward0]
	2165916735360 -> 2165916735264
	2165916735360 [label=ConvolutionBackward0]
	2165916735552 -> 2165916735360
	2165916735552 [label=UpsampleNearest2DBackward1]
	2165916735696 -> 2165916735552
	2165916735696 [label=ViewBackward0]
	2165916735792 -> 2165916735696
	2165916735792 [label=AddmmBackward0]
	2165916735840 -> 2165916735792
	2165915555680 [label="decoder.linear.bias
 (8192)" fillcolor=lightblue]
	2165915555680 -> 2165916735840
	2165916735840 [label=AccumulateGrad]
	2165916735600 -> 2165916735792
	2165916735600 [label=AddBackward0]
	2165916736032 -> 2165916735600
	2165916736032 [label=AddmmBackward0]
	2165916736368 -> 2165916736032
	2165916572768 [label="fc_mu.bias
 (256)" fillcolor=lightblue]
	2165916572768 -> 2165916736368
	2165916736368 [label=AccumulateGrad]
	2165916736320 -> 2165916736032
	2165916736320 [label=ReshapeAliasBackward0]
	2165916736464 -> 2165916736320
	2165916736464 [label=MeanBackward1]
	2165916736656 -> 2165916736464
	2165916736656 [label=ReluBackward0]
	2165916736752 -> 2165916736656
	2165916736752 [label=AddBackward0]
	2165916736800 -> 2165916736752
	2165916736800 [label=NativeBatchNormBackward0]
	2165916737040 -> 2165916736800
	2165916737040 [label=ConvolutionBackward0]
	2165916737232 -> 2165916737040
	2165916737232 [label=ReluBackward0]
	2165916737376 -> 2165916737232
	2165916737376 [label=NativeBatchNormBackward0]
	2165916737424 -> 2165916737376
	2165916737424 [label=ConvolutionBackward0]
	2165916736560 -> 2165916737424
	2165916736560 [label=ReluBackward0]
	2165916737808 -> 2165916736560
	2165916737808 [label=AddBackward0]
	2165916737856 -> 2165916737808
	2165916737856 [label=NativeBatchNormBackward0]
	2165916738096 -> 2165916737856
	2165916738096 [label=ConvolutionBackward0]
	2165916738288 -> 2165916738096
	2165916738288 [label=ReluBackward0]
	2165916738432 -> 2165916738288
	2165916738432 [label=NativeBatchNormBackward0]
	2165916738480 -> 2165916738432
	2165916738480 [label=ConvolutionBackward0]
	2165916738768 -> 2165916738480
	2165916738768 [label=ReluBackward0]
	2165916738912 -> 2165916738768
	2165916738912 [label=AddBackward0]
	2165916738960 -> 2165916738912
	2165916738960 [label=NativeBatchNormBackward0]
	2165916739200 -> 2165916738960
	2165916739200 [label=ConvolutionBackward0]
	2165916739392 -> 2165916739200
	2165916739392 [label=ReluBackward0]
	2165916739536 -> 2165916739392
	2165916739536 [label=NativeBatchNormBackward0]
	2165916739584 -> 2165916739536
	2165916739584 [label=ConvolutionBackward0]
	2165916738816 -> 2165916739584
	2165916738816 [label=ReluBackward0]
	2165916739968 -> 2165916738816
	2165916739968 [label=AddBackward0]
	2165916740016 -> 2165916739968
	2165916740016 [label=NativeBatchNormBackward0]
	2165916740256 -> 2165916740016
	2165916740256 [label=ConvolutionBackward0]
	2165916740448 -> 2165916740256
	2165916740448 [label=ReluBackward0]
	2165916740592 -> 2165916740448
	2165916740592 [label=NativeBatchNormBackward0]
	2165916740640 -> 2165916740592
	2165916740640 [label=ConvolutionBackward0]
	2165916740928 -> 2165916740640
	2165916740928 [label=ReluBackward0]
	2165916741072 -> 2165916740928
	2165916741072 [label=AddBackward0]
	2165916741120 -> 2165916741072
	2165916741120 [label=NativeBatchNormBackward0]
	2165916741360 -> 2165916741120
	2165916741360 [label=ConvolutionBackward0]
	2165916741552 -> 2165916741360
	2165916741552 [label=ReluBackward0]
	2165916741696 -> 2165916741552
	2165916741696 [label=NativeBatchNormBackward0]
	2165916741744 -> 2165916741696
	2165916741744 [label=ConvolutionBackward0]
	2165916740976 -> 2165916741744
	2165916740976 [label=ReluBackward0]
	2165916742128 -> 2165916740976
	2165916742128 [label=AddBackward0]
	2165916742176 -> 2165916742128
	2165916742176 [label=NativeBatchNormBackward0]
	2165916742416 -> 2165916742176
	2165916742416 [label=ConvolutionBackward0]
	2165916742608 -> 2165916742416
	2165916742608 [label=ReluBackward0]
	2165916742752 -> 2165916742608
	2165916742752 [label=NativeBatchNormBackward0]
	2165916742800 -> 2165916742752
	2165916742800 [label=ConvolutionBackward0]
	2165916743088 -> 2165916742800
	2165916743088 [label=ReluBackward0]
	2165916743232 -> 2165916743088
	2165916743232 [label=AddBackward0]
	2165916743280 -> 2165916743232
	2165916743280 [label=NativeBatchNormBackward0]
	2165916743520 -> 2165916743280
	2165916743520 [label=ConvolutionBackward0]
	2165916743712 -> 2165916743520
	2165916743712 [label=ReluBackward0]
	2165916743856 -> 2165916743712
	2165916743856 [label=NativeBatchNormBackward0]
	2165916743904 -> 2165916743856
	2165916743904 [label=ConvolutionBackward0]
	2165916743136 -> 2165916743904
	2165916743136 [label=ReluBackward0]
	2165916744288 -> 2165916743136
	2165916744288 [label=AddBackward0]
	2165916744336 -> 2165916744288
	2165916744336 [label=NativeBatchNormBackward0]
	2165916744576 -> 2165916744336
	2165916744576 [label=ConvolutionBackward0]
	2165916744768 -> 2165916744576
	2165916744768 [label=ReluBackward0]
	2165916744912 -> 2165916744768
	2165916744912 [label=NativeBatchNormBackward0]
	2165916744960 -> 2165916744912
	2165916744960 [label=ConvolutionBackward0]
	2165916744096 -> 2165916744960
	2165916744096 [label=MaxPool2DWithIndicesBackward0]
	2165916745344 -> 2165916744096
	2165916745344 [label=ReluBackward0]
	2165916745392 -> 2165916745344
	2165916745392 [label=NativeBatchNormBackward0]
	2165916745536 -> 2165916745392
	2165916745536 [label=ConvolutionBackward0]
	2165916795040 -> 2165916745536
	2165915015248 [label="encoder.conv1.weight
 (64, 3, 3, 3)" fillcolor=lightblue]
	2165915015248 -> 2165916795040
	2165916795040 [label=AccumulateGrad]
	2165916745488 -> 2165916745392
	2165915552880 [label="encoder.bn1.weight
 (64)" fillcolor=lightblue]
	2165915552880 -> 2165916745488
	2165916745488 [label=AccumulateGrad]
	2165916745632 -> 2165916745392
	2165916303424 [label="encoder.bn1.bias
 (64)" fillcolor=lightblue]
	2165916303424 -> 2165916745632
	2165916745632 [label=AccumulateGrad]
	2165916745248 -> 2165916744960
	2165916303744 [label="encoder.layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2165916303744 -> 2165916745248
	2165916745248 [label=AccumulateGrad]
	2165916744816 -> 2165916744912
	2165916303904 [label="encoder.layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2165916303904 -> 2165916744816
	2165916744816 [label=AccumulateGrad]
	2165916745056 -> 2165916744912
	2165916303984 [label="encoder.layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2165916303984 -> 2165916745056
	2165916745056 [label=AccumulateGrad]
	2165916744720 -> 2165916744576
	2165916304464 [label="encoder.layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2165916304464 -> 2165916744720
	2165916744720 [label=AccumulateGrad]
	2165916744528 -> 2165916744336
	2165916304544 [label="encoder.layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2165916304544 -> 2165916744528
	2165916744528 [label=AccumulateGrad]
	2165916744480 -> 2165916744336
	2165916304624 [label="encoder.layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2165916304624 -> 2165916744480
	2165916744480 [label=AccumulateGrad]
	2165916744096 -> 2165916744288
	2165916744192 -> 2165916743904
	2165916305104 [label="encoder.layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2165916305104 -> 2165916744192
	2165916744192 [label=AccumulateGrad]
	2165916743760 -> 2165916743856
	2165916305184 [label="encoder.layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2165916305184 -> 2165916743760
	2165916743760 [label=AccumulateGrad]
	2165916744000 -> 2165916743856
	2165916305264 [label="encoder.layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2165916305264 -> 2165916744000
	2165916744000 [label=AccumulateGrad]
	2165916743664 -> 2165916743520
	2165916305744 [label="encoder.layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2165916305744 -> 2165916743664
	2165916743664 [label=AccumulateGrad]
	2165916743472 -> 2165916743280
	2165916305824 [label="encoder.layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2165916305824 -> 2165916743472
	2165916743472 [label=AccumulateGrad]
	2165916743424 -> 2165916743280
	2165916305904 [label="encoder.layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2165916305904 -> 2165916743424
	2165916743424 [label=AccumulateGrad]
	2165916743136 -> 2165916743232
	2165916743040 -> 2165916742800
	2165916306864 [label="encoder.layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2165916306864 -> 2165916743040
	2165916743040 [label=AccumulateGrad]
	2165916742656 -> 2165916742752
	2165916306944 [label="encoder.layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2165916306944 -> 2165916742656
	2165916742656 [label=AccumulateGrad]
	2165916742896 -> 2165916742752
	2165916307024 [label="encoder.layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2165916307024 -> 2165916742896
	2165916742896 [label=AccumulateGrad]
	2165916742560 -> 2165916742416
	2165916307504 [label="encoder.layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2165916307504 -> 2165916742560
	2165916742560 [label=AccumulateGrad]
	2165916742368 -> 2165916742176
	2165916307584 [label="encoder.layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2165916307584 -> 2165916742368
	2165916742368 [label=AccumulateGrad]
	2165916742320 -> 2165916742176
	2165916307664 [label="encoder.layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2165916307664 -> 2165916742320
	2165916742320 [label=AccumulateGrad]
	2165916741936 -> 2165916742128
	2165916741936 [label=NativeBatchNormBackward0]
	2165916742992 -> 2165916741936
	2165916742992 [label=ConvolutionBackward0]
	2165916743088 -> 2165916742992
	2165916743376 -> 2165916742992
	2165916306224 [label="encoder.layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	2165916306224 -> 2165916743376
	2165916743376 [label=AccumulateGrad]
	2165916742512 -> 2165916741936
	2165916306304 [label="encoder.layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	2165916306304 -> 2165916742512
	2165916742512 [label=AccumulateGrad]
	2165916742464 -> 2165916741936
	2165916306384 [label="encoder.layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	2165916306384 -> 2165916742464
	2165916742464 [label=AccumulateGrad]
	2165916742032 -> 2165916741744
	2165916308144 [label="encoder.layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2165916308144 -> 2165916742032
	2165916742032 [label=AccumulateGrad]
	2165916741600 -> 2165916741696
	2165916308224 [label="encoder.layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2165916308224 -> 2165916741600
	2165916741600 [label=AccumulateGrad]
	2165916741840 -> 2165916741696
	2165916308304 [label="encoder.layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2165916308304 -> 2165916741840
	2165916741840 [label=AccumulateGrad]
	2165916741504 -> 2165916741360
	2165916308704 [label="encoder.layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2165916308704 -> 2165916741504
	2165916741504 [label=AccumulateGrad]
	2165916741312 -> 2165916741120
	2165916308784 [label="encoder.layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2165916308784 -> 2165916741312
	2165916741312 [label=AccumulateGrad]
	2165916741264 -> 2165916741120
	2165916308864 [label="encoder.layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2165916308864 -> 2165916741264
	2165916741264 [label=AccumulateGrad]
	2165916740976 -> 2165916741072
	2165916740880 -> 2165916740640
	2165916309904 [label="encoder.layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2165916309904 -> 2165916740880
	2165916740880 [label=AccumulateGrad]
	2165916740496 -> 2165916740592
	2165916309984 [label="encoder.layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2165916309984 -> 2165916740496
	2165916740496 [label=AccumulateGrad]
	2165916740736 -> 2165916740592
	2165916310064 [label="encoder.layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2165916310064 -> 2165916740736
	2165916740736 [label=AccumulateGrad]
	2165916740400 -> 2165916740256
	2165916310544 [label="encoder.layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2165916310544 -> 2165916740400
	2165916740400 [label=AccumulateGrad]
	2165916740208 -> 2165916740016
	2165916310624 [label="encoder.layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2165916310624 -> 2165916740208
	2165916740208 [label=AccumulateGrad]
	2165916740160 -> 2165916740016
	2165916310704 [label="encoder.layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2165916310704 -> 2165916740160
	2165916740160 [label=AccumulateGrad]
	2165916739776 -> 2165916739968
	2165916739776 [label=NativeBatchNormBackward0]
	2165916740832 -> 2165916739776
	2165916740832 [label=ConvolutionBackward0]
	2165916740928 -> 2165916740832
	2165916741216 -> 2165916740832
	2165916309264 [label="encoder.layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	2165916309264 -> 2165916741216
	2165916741216 [label=AccumulateGrad]
	2165916740352 -> 2165916739776
	2165916309344 [label="encoder.layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2165916309344 -> 2165916740352
	2165916740352 [label=AccumulateGrad]
	2165916740304 -> 2165916739776
	2165916309424 [label="encoder.layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2165916309424 -> 2165916740304
	2165916740304 [label=AccumulateGrad]
	2165916739872 -> 2165916739584
	2165916311104 [label="encoder.layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2165916311104 -> 2165916739872
	2165916739872 [label=AccumulateGrad]
	2165916739440 -> 2165916739536
	2165916311184 [label="encoder.layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2165916311184 -> 2165916739440
	2165916739440 [label=AccumulateGrad]
	2165916739680 -> 2165916739536
	2165916311264 [label="encoder.layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2165916311264 -> 2165916739680
	2165916739680 [label=AccumulateGrad]
	2165916739344 -> 2165916739200
	2165916311744 [label="encoder.layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2165916311744 -> 2165916739344
	2165916739344 [label=AccumulateGrad]
	2165916739152 -> 2165916738960
	2165916311824 [label="encoder.layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2165916311824 -> 2165916739152
	2165916739152 [label=AccumulateGrad]
	2165916739104 -> 2165916738960
	2165916311904 [label="encoder.layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2165916311904 -> 2165916739104
	2165916739104 [label=AccumulateGrad]
	2165916738816 -> 2165916738912
	2165916738720 -> 2165916738480
	2165916313024 [label="encoder.layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2165916313024 -> 2165916738720
	2165916738720 [label=AccumulateGrad]
	2165916738336 -> 2165916738432
	2165916313104 [label="encoder.layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2165916313104 -> 2165916738336
	2165916738336 [label=AccumulateGrad]
	2165916738576 -> 2165916738432
	2165916313184 [label="encoder.layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2165916313184 -> 2165916738576
	2165916738576 [label=AccumulateGrad]
	2165916738240 -> 2165916738096
	2165916313664 [label="encoder.layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2165916313664 -> 2165916738240
	2165916738240 [label=AccumulateGrad]
	2165916738048 -> 2165916737856
	2165916313744 [label="encoder.layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2165916313744 -> 2165916738048
	2165916738048 [label=AccumulateGrad]
	2165916738000 -> 2165916737856
	2165916313824 [label="encoder.layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2165916313824 -> 2165916738000
	2165916738000 [label=AccumulateGrad]
	2165916737616 -> 2165916737808
	2165916737616 [label=NativeBatchNormBackward0]
	2165916738672 -> 2165916737616
	2165916738672 [label=ConvolutionBackward0]
	2165916738768 -> 2165916738672
	2165916739248 -> 2165916738672
	2165916312384 [label="encoder.layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2165916312384 -> 2165916739248
	2165916739248 [label=AccumulateGrad]
	2165916738192 -> 2165916737616
	2165916312464 [label="encoder.layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2165916312464 -> 2165916738192
	2165916738192 [label=AccumulateGrad]
	2165916738144 -> 2165916737616
	2165916312544 [label="encoder.layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2165916312544 -> 2165916738144
	2165916738144 [label=AccumulateGrad]
	2165916737712 -> 2165916737424
	2165916314304 [label="encoder.layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2165916314304 -> 2165916737712
	2165916737712 [label=AccumulateGrad]
	2165916737280 -> 2165916737376
	2165916314384 [label="encoder.layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2165916314384 -> 2165916737280
	2165916737280 [label=AccumulateGrad]
	2165916737520 -> 2165916737376
	2165916314464 [label="encoder.layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2165916314464 -> 2165916737520
	2165916737520 [label=AccumulateGrad]
	2165916737184 -> 2165916737040
	2165916314944 [label="encoder.layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2165916314944 -> 2165916737184
	2165916737184 [label=AccumulateGrad]
	2165916736992 -> 2165916736800
	2165916315024 [label="encoder.layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2165916315024 -> 2165916736992
	2165916736992 [label=AccumulateGrad]
	2165916736944 -> 2165916736800
	2165916315104 [label="encoder.layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2165916315104 -> 2165916736944
	2165916736944 [label=AccumulateGrad]
	2165916736560 -> 2165916736752
	2165916736272 -> 2165916736032
	2165916736272 [label=TBackward0]
	2165916736704 -> 2165916736272
	2165916572688 [label="fc_mu.weight
 (256, 512)" fillcolor=lightblue]
	2165916572688 -> 2165916736704
	2165916736704 [label=AccumulateGrad]
	2165916736176 -> 2165916735600
	2165916736176 [label=MulBackward0]
	2165916737088 -> 2165916736176
	2165916737088 [label=ExpBackward0]
	2165916736512 -> 2165916737088
	2165916736512 [label=DivBackward0]
	2165916737664 -> 2165916736512
	2165916737664 [label=AddmmBackward0]
	2165916737328 -> 2165916737664
	2165916572928 [label="fc_var.bias
 (256)" fillcolor=lightblue]
	2165916572928 -> 2165916737328
	2165916737328 [label=AccumulateGrad]
	2165916736320 -> 2165916737664
	2165916737568 -> 2165916737664
	2165916737568 [label=TBackward0]
	2165916738624 -> 2165916737568
	2165916572848 [label="fc_var.weight
 (256, 512)" fillcolor=lightblue]
	2165916572848 -> 2165916738624
	2165916738624 [label=AccumulateGrad]
	2165916735936 -> 2165916735792
	2165916735936 [label=TBackward0]
	2165916736896 -> 2165916735936
	2165915549760 [label="decoder.linear.weight
 (8192, 256)" fillcolor=lightblue]
	2165915549760 -> 2165916736896
	2165916736896 [label=AccumulateGrad]
	2165916735504 -> 2165916735360
	2165916315904 [label="decoder.layer1.0.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2165916315904 -> 2165916735504
	2165916735504 [label=AccumulateGrad]
	2165916735312 -> 2165916735264
	2165916315984 [label="decoder.layer1.0.bn1.weight
 (512)" fillcolor=lightblue]
	2165916315984 -> 2165916735312
	2165916735312 [label=AccumulateGrad]
	2165916735072 -> 2165916735264
	2165916316064 [label="decoder.layer1.0.bn1.bias
 (512)" fillcolor=lightblue]
	2165916316064 -> 2165916735072
	2165916735072 [label=AccumulateGrad]
	2165916734976 -> 2165916734832
	2165916316544 [label="decoder.layer1.0.conv2.1.weight
 (256, 512, 3, 3)" fillcolor=lightblue]
	2165916316544 -> 2165916734976
	2165916734976 [label=AccumulateGrad]
	2165916734784 -> 2165916734688
	2165916316624 [label="decoder.layer1.0.bn2.weight
 (256)" fillcolor=lightblue]
	2165916316624 -> 2165916734784
	2165916734784 [label=AccumulateGrad]
	2165916734736 -> 2165916734688
	2165916316704 [label="decoder.layer1.0.bn2.bias
 (256)" fillcolor=lightblue]
	2165916316704 -> 2165916734736
	2165916734736 [label=AccumulateGrad]
	2165916734640 -> 2165916734592
	2165916734640 [label=NativeBatchNormBackward0]
	2165916735408 -> 2165916734640
	2165916735408 [label=ConvolutionBackward0]
	2165916735984 -> 2165916735408
	2165916735984 [label=UpsampleNearest2DBackward1]
	2165916735552 -> 2165916735984
	2165916735456 -> 2165916735408
	2165914702752 [label="decoder.layer1.0.upsample.0.1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	2165914702752 -> 2165916735456
	2165916735456 [label=AccumulateGrad]
	2165916734928 -> 2165916734640
	2165916315424 [label="decoder.layer1.0.upsample.1.weight
 (256)" fillcolor=lightblue]
	2165916315424 -> 2165916734928
	2165916734928 [label=AccumulateGrad]
	2165916734880 -> 2165916734640
	2165916315504 [label="decoder.layer1.0.upsample.1.bias
 (256)" fillcolor=lightblue]
	2165916315504 -> 2165916734880
	2165916734880 [label=AccumulateGrad]
	2165916734496 -> 2165916734304
	2165916317184 [label="decoder.layer1.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2165916317184 -> 2165916734496
	2165916734496 [label=AccumulateGrad]
	2165916734256 -> 2165916734208
	2165916317264 [label="decoder.layer1.1.bn1.weight
 (256)" fillcolor=lightblue]
	2165916317264 -> 2165916734256
	2165916734256 [label=AccumulateGrad]
	2165916734112 -> 2165916734208
	2165916317344 [label="decoder.layer1.1.bn1.bias
 (256)" fillcolor=lightblue]
	2165916317344 -> 2165916734112
	2165916734112 [label=AccumulateGrad]
	2165916734016 -> 2165916733872
	2165916317824 [label="decoder.layer1.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2165916317824 -> 2165916734016
	2165916734016 [label=AccumulateGrad]
	2165916733824 -> 2165916733728
	2165916317904 [label="decoder.layer1.1.bn2.weight
 (256)" fillcolor=lightblue]
	2165916317904 -> 2165916733824
	2165916733824 [label=AccumulateGrad]
	2165916733776 -> 2165916733728
	2165916317984 [label="decoder.layer1.1.bn2.bias
 (256)" fillcolor=lightblue]
	2165916317984 -> 2165916733776
	2165916733776 [label=AccumulateGrad]
	2165916733680 -> 2165916733632
	2165916733440 -> 2165916733296
	2165916318944 [label="decoder.layer2.0.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2165916318944 -> 2165916733440
	2165916733440 [label=AccumulateGrad]
	2165916733248 -> 2165916733200
	2165916319024 [label="decoder.layer2.0.bn1.weight
 (256)" fillcolor=lightblue]
	2165916319024 -> 2165916733248
	2165916733248 [label=AccumulateGrad]
	2165916733008 -> 2165916733200
	2165916319104 [label="decoder.layer2.0.bn1.bias
 (256)" fillcolor=lightblue]
	2165916319104 -> 2165916733008
	2165916733008 [label=AccumulateGrad]
	2165916732912 -> 2165916732768
	2165916319584 [label="decoder.layer2.0.conv2.1.weight
 (128, 256, 3, 3)" fillcolor=lightblue]
	2165916319584 -> 2165916732912
	2165916732912 [label=AccumulateGrad]
	2165916732720 -> 2165916732624
	2165916319664 [label="decoder.layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2165916319664 -> 2165916732720
	2165916732720 [label=AccumulateGrad]
	2165916732672 -> 2165916732624
	2165916565568 [label="decoder.layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2165916565568 -> 2165916732672
	2165916732672 [label=AccumulateGrad]
	2165916732576 -> 2165916732528
	2165916732576 [label=NativeBatchNormBackward0]
	2165916733344 -> 2165916732576
	2165916733344 [label=ConvolutionBackward0]
	2165916733920 -> 2165916733344
	2165916733920 [label=UpsampleNearest2DBackward1]
	2165916733488 -> 2165916733920
	2165916733392 -> 2165916733344
	2165916318464 [label="decoder.layer2.0.upsample.0.1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	2165916318464 -> 2165916733392
	2165916733392 [label=AccumulateGrad]
	2165916732864 -> 2165916732576
	2165916318384 [label="decoder.layer2.0.upsample.1.weight
 (128)" fillcolor=lightblue]
	2165916318384 -> 2165916732864
	2165916732864 [label=AccumulateGrad]
	2165916732816 -> 2165916732576
	2165916318544 [label="decoder.layer2.0.upsample.1.bias
 (128)" fillcolor=lightblue]
	2165916318544 -> 2165916732816
	2165916732816 [label=AccumulateGrad]
	2165916732432 -> 2165916732240
	2165916565968 [label="decoder.layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2165916565968 -> 2165916732432
	2165916732432 [label=AccumulateGrad]
	2165916732192 -> 2165916732144
	2165916566048 [label="decoder.layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2165916566048 -> 2165916732192
	2165916732192 [label=AccumulateGrad]
	2165916732048 -> 2165916732144
	2165916566128 [label="decoder.layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2165916566128 -> 2165916732048
	2165916732048 [label=AccumulateGrad]
	2165916731952 -> 2165916731808
	2165916566528 [label="decoder.layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2165916566528 -> 2165916731952
	2165916731952 [label=AccumulateGrad]
	2165916731760 -> 2165916731664
	2165916566608 [label="decoder.layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2165916566608 -> 2165916731760
	2165916731760 [label=AccumulateGrad]
	2165916731712 -> 2165916731664
	2165916566688 [label="decoder.layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2165916566688 -> 2165916731712
	2165916731712 [label=AccumulateGrad]
	2165916731616 -> 2165916731568
	2165916731376 -> 2165916731232
	2165916567728 [label="decoder.layer3.0.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2165916567728 -> 2165916731376
	2165916731376 [label=AccumulateGrad]
	2165916731184 -> 2165916731136
	2165916567808 [label="decoder.layer3.0.bn1.weight
 (128)" fillcolor=lightblue]
	2165916567808 -> 2165916731184
	2165916731184 [label=AccumulateGrad]
	2165916730944 -> 2165916731136
	2165916567888 [label="decoder.layer3.0.bn1.bias
 (128)" fillcolor=lightblue]
	2165916567888 -> 2165916730944
	2165916730944 [label=AccumulateGrad]
	2165916730848 -> 2165916730704
	2165916568368 [label="decoder.layer3.0.conv2.1.weight
 (64, 128, 3, 3)" fillcolor=lightblue]
	2165916568368 -> 2165916730848
	2165916730848 [label=AccumulateGrad]
	2165916730656 -> 2165916730560
	2165916568448 [label="decoder.layer3.0.bn2.weight
 (64)" fillcolor=lightblue]
	2165916568448 -> 2165916730656
	2165916730656 [label=AccumulateGrad]
	2165916730608 -> 2165916730560
	2165916568528 [label="decoder.layer3.0.bn2.bias
 (64)" fillcolor=lightblue]
	2165916568528 -> 2165916730608
	2165916730608 [label=AccumulateGrad]
	2165916730512 -> 2165916730464
	2165916730512 [label=NativeBatchNormBackward0]
	2165916731280 -> 2165916730512
	2165916731280 [label=ConvolutionBackward0]
	2165916731856 -> 2165916731280
	2165916731856 [label=UpsampleNearest2DBackward1]
	2165916731424 -> 2165916731856
	2165916731328 -> 2165916731280
	2165916567168 [label="decoder.layer3.0.upsample.0.1.weight
 (64, 128, 1, 1)" fillcolor=lightblue]
	2165916567168 -> 2165916731328
	2165916731328 [label=AccumulateGrad]
	2165916730800 -> 2165916730512
	2165916567088 [label="decoder.layer3.0.upsample.1.weight
 (64)" fillcolor=lightblue]
	2165916567088 -> 2165916730800
	2165916730800 [label=AccumulateGrad]
	2165916730752 -> 2165916730512
	2165916567248 [label="decoder.layer3.0.upsample.1.bias
 (64)" fillcolor=lightblue]
	2165916567248 -> 2165916730752
	2165916730752 [label=AccumulateGrad]
	2165916730368 -> 2165916730176
	2165916568928 [label="decoder.layer3.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2165916568928 -> 2165916730368
	2165916730368 [label=AccumulateGrad]
	2165916730128 -> 2165916730080
	2165916569008 [label="decoder.layer3.1.bn1.weight
 (64)" fillcolor=lightblue]
	2165916569008 -> 2165916730128
	2165916730128 [label=AccumulateGrad]
	2165916729984 -> 2165916730080
	2165916569088 [label="decoder.layer3.1.bn1.bias
 (64)" fillcolor=lightblue]
	2165916569088 -> 2165916729984
	2165916729984 [label=AccumulateGrad]
	2165916729888 -> 2165916729744
	2165916569568 [label="decoder.layer3.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2165916569568 -> 2165916729888
	2165916729888 [label=AccumulateGrad]
	2165916729696 -> 2165916729600
	2165916569648 [label="decoder.layer3.1.bn2.weight
 (64)" fillcolor=lightblue]
	2165916569648 -> 2165916729696
	2165916729696 [label=AccumulateGrad]
	2165916729648 -> 2165916729600
	2165916569728 [label="decoder.layer3.1.bn2.bias
 (64)" fillcolor=lightblue]
	2165916569728 -> 2165916729648
	2165916729648 [label=AccumulateGrad]
	2165916729552 -> 2165916729504
	2165916205008 -> 2165916204864
	2165916570208 [label="decoder.layer4.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2165916570208 -> 2165916205008
	2165916205008 [label=AccumulateGrad]
	2165916204816 -> 2165916204768
	2165916570288 [label="decoder.layer4.0.bn1.weight
 (64)" fillcolor=lightblue]
	2165916570288 -> 2165916204816
	2165916204816 [label=AccumulateGrad]
	2165916204672 -> 2165916204768
	2165916570368 [label="decoder.layer4.0.bn1.bias
 (64)" fillcolor=lightblue]
	2165916570368 -> 2165916204672
	2165916204672 [label=AccumulateGrad]
	2165916204576 -> 2165916204432
	2165916570848 [label="decoder.layer4.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2165916570848 -> 2165916204576
	2165916204576 [label=AccumulateGrad]
	2165916204384 -> 2165916204288
	2165916570928 [label="decoder.layer4.0.bn2.weight
 (64)" fillcolor=lightblue]
	2165916570928 -> 2165916204384
	2165916204384 [label=AccumulateGrad]
	2165916204336 -> 2165916204288
	2165916571008 [label="decoder.layer4.0.bn2.bias
 (64)" fillcolor=lightblue]
	2165916571008 -> 2165916204336
	2165916204336 [label=AccumulateGrad]
	2165916204240 -> 2165916204192
	2165916204096 -> 2165916203904
	2165916571408 [label="decoder.layer4.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2165916571408 -> 2165916204096
	2165916204096 [label=AccumulateGrad]
	2165916203856 -> 2165916203808
	2165916571488 [label="decoder.layer4.1.bn1.weight
 (64)" fillcolor=lightblue]
	2165916571488 -> 2165916203856
	2165916203856 [label=AccumulateGrad]
	2165916203712 -> 2165916203808
	2165916571568 [label="decoder.layer4.1.bn1.bias
 (64)" fillcolor=lightblue]
	2165916571568 -> 2165916203712
	2165916203712 [label=AccumulateGrad]
	2165916203616 -> 2165916203472
	2165916571968 [label="decoder.layer4.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2165916571968 -> 2165916203616
	2165916203616 [label=AccumulateGrad]
	2165916203424 -> 2165916203328
	2165916572048 [label="decoder.layer4.1.bn2.weight
 (64)" fillcolor=lightblue]
	2165916572048 -> 2165916203424
	2165916203424 [label=AccumulateGrad]
	2165916203376 -> 2165916203328
	2165916572128 [label="decoder.layer4.1.bn2.bias
 (64)" fillcolor=lightblue]
	2165916572128 -> 2165916203376
	2165916203376 [label=AccumulateGrad]
	2165916203280 -> 2165916203232
	2165916203088 -> 2165916203040
	2165916572608 [label="decoder.conv1.weight
 (3, 64, 3, 3)" fillcolor=lightblue]
	2165916572608 -> 2165916203088
	2165916203088 [label=AccumulateGrad]
	2165916202752 -> 2165915411424
}
